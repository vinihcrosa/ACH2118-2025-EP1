# Modelos Word2Vec - Guia Completo

## ğŸ“‹ Resumo

Foram adicionados **2 novos modelos** ao sistema de pipelines configurÃ¡veis via JSON:
- **`word2vec_lr`**: Word2Vec + Logistic Regression
- **`word2vec_svc`**: Word2Vec + Linear SVC

## ğŸ¯ O que Ã© Word2Vec?

Word2Vec Ã© uma tÃ©cnica de **embeddings de palavras** que representa cada palavra como um vetor numÃ©rico denso. Diferente do TF-IDF que cria vetores esparsos baseados em frequÃªncias, Word2Vec aprende representaÃ§Ãµes semÃ¢nticas das palavras.

### Vantagens sobre TF-IDF:
- âœ… Captura **relaÃ§Ãµes semÃ¢nticas** (palavras similares tÃªm vetores prÃ³ximos)
- âœ… Vetores **densos** (menor dimensionalidade que TF-IDF)
- âœ… Funciona melhor com **corpus menores**
- âœ… Pode capturar **contexto** das palavras

### Desvantagens:
- âŒ Mais **lento** para treinar
- âŒ Requer mais **memÃ³ria**
- âŒ NÃ£o Ã© **interpretÃ¡vel** como TF-IDF

## ğŸ”§ ConfiguraÃ§Ã£o no JSON

Os modelos foram adicionados em `config/pipelines.json`:

```json
{
  "models": {
    "word2vec_lr": {
      "type": "custom",
      "factory": "src.ep1.models.word2vec.create_word2vec_lr",
      "params": {}
    },
    "word2vec_svc": {
      "type": "custom",
      "factory": "src.ep1.models.word2vec.create_word2vec_svc",
      "params": {}
    }
  },
  "experiments": [
    {
      "model": "word2vec_lr",
      "param_grid": {
        "word2vec__vector_size": [50, 100, 200],
        "word2vec__window": [3, 5, 7],
        "word2vec__sg": [0, 1],
        "clf__C": [0.1, 1.0, 10.0]
      }
    },
    {
      "model": "word2vec_svc",
      "param_grid": {
        "word2vec__vector_size": [50, 100, 200],
        "word2vec__window": [3, 5, 7],
        "word2vec__sg": [0, 1],
        "clf__C": [0.1, 1.0, 10.0]
      }
    }
  ]
}
```

## ğŸ“Š ParÃ¢metros do Word2Vec

### `word2vec__vector_size`
- **DescriÃ§Ã£o**: Dimensionalidade dos vetores de palavras
- **Valores**: `[50, 100, 200]`
- **RecomendaÃ§Ã£o**: 
  - 50: datasets pequenos, treinamento rÃ¡pido
  - 100: balanceado (padrÃ£o)
  - 200: datasets grandes, melhor qualidade

### `word2vec__window`
- **DescriÃ§Ã£o**: Janela de contexto (quantas palavras antes/depois considerar)
- **Valores**: `[3, 5, 7]`
- **RecomendaÃ§Ã£o**:
  - 3: contexto local
  - 5: balanceado (padrÃ£o)
  - 7: contexto mais amplo

### `word2vec__sg`
- **DescriÃ§Ã£o**: Algoritmo de treinamento
- **Valores**: 
  - `0`: CBOW (Continuous Bag of Words) - mais rÃ¡pido
  - `1`: Skip-gram - melhor para palavras raras
- **RecomendaÃ§Ã£o**: 
  - CBOW (0) para datasets grandes
  - Skip-gram (1) para datasets pequenos

### `clf__C`
- **DescriÃ§Ã£o**: ParÃ¢metro de regularizaÃ§Ã£o do classificador
- **Valores**: `[0.1, 1.0, 10.0]`
- **RecomendaÃ§Ã£o**:
  - 0.1: mais regularizaÃ§Ã£o
  - 1.0: balanceado
  - 10.0: menos regularizaÃ§Ã£o

## ğŸš€ Como Usar

### 1. Treinar um modelo especÃ­fico

```bash
uv run python src/ep1/train.py --dataset arcaico_moderno --model word2vec_lr --save
```

### 2. Rodar experimentos com grid search

```bash
# Apenas Word2Vec LR
uv run python src/ep1/experiments.py --models word2vec_lr

# Apenas Word2Vec SVC
uv run python src/ep1/experiments.py --models word2vec_svc

# Ambos os modelos Word2Vec
uv run python src/ep1/experiments.py --models word2vec_lr word2vec_svc

# Todos os modelos (incluindo Word2Vec)
uv run python src/ep1/experiments.py
```

### 3. Usar modelo treinado para prediÃ§Ã£o

```bash
# Listar modelos disponÃ­veis
uv run python predict_cli.py --list-models

# Fazer prediÃ§Ã£o
uv run python predict_cli.py \
  --model arcaico_moderno__word2vec_lr.joblib \
  --text "Texto para classificar" \
  --proba
```

### 4. Usar no cÃ³digo Python

```python
from src.ep1.config import get_default_config

# Carregar configuraÃ§Ã£o
config = get_default_config()

# Criar modelo
model = config.get_model('word2vec_lr')

# Treinar
X_train = ["texto 1", "texto 2", ...]
y_train = ["classe 1", "classe 2", ...]
model.fit(X_train, y_train)

# Predizer
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)

# Salvar modelo
import joblib
joblib.dump(model, 'models/meu_word2vec.joblib')
```

## ğŸ“ˆ NÃºmero de Experimentos

Cada modelo Word2Vec tem um grid de parÃ¢metros com:
- 3 valores de `vector_size`
- 3 valores de `window`
- 2 valores de `sg`
- 3 valores de `C`

**Total**: 3 Ã— 3 Ã— 2 Ã— 3 = **54 combinaÃ§Ãµes por modelo**

Com 3 datasets e 2 modelos Word2Vec:
- **word2vec_lr**: 54 Ã— 3 = 162 experimentos
- **word2vec_svc**: 54 Ã— 3 = 162 experimentos
- **Total**: 324 experimentos

## ğŸ” ImplementaÃ§Ã£o

O cÃ³digo estÃ¡ em `src/ep1/models/word2vec.py`:

```python
class Word2VecVectorizer(BaseEstimator, TransformerMixin):
    """
    Vetorizador compatÃ­vel com sklearn que usa Word2Vec.
    
    Para cada documento:
    1. Tokeniza o texto em palavras
    2. ObtÃ©m o embedding de cada palavra
    3. Calcula a mÃ©dia dos embeddings
    """
    
    def __init__(self, vector_size=100, window=5, min_count=1, 
                 workers=4, sg=0, epochs=10):
        ...
    
    def fit(self, X, y=None):
        # Treina o modelo Word2Vec
        ...
    
    def transform(self, X):
        # Transforma documentos em vetores (mÃ©dia dos word embeddings)
        ...
```

## ğŸ’¡ Exemplos PrÃ¡ticos

### Exemplo 1: Teste rÃ¡pido

```bash
uv run python example_word2vec.py
```

Este script:
- Carrega um dataset
- Treina Word2Vec + LR e Word2Vec + SVC
- Compara prediÃ§Ãµes
- Mostra componentes do pipeline

### Exemplo 2: Grid search completo

```bash
# Rodar todos os experimentos Word2Vec (pode demorar!)
uv run python src/ep1/experiments.py --models word2vec_lr word2vec_svc
```

### Exemplo 3: Comparar com TF-IDF

```bash
# Treinar TF-IDF + LR
uv run python src/ep1/train.py --dataset arcaico_moderno --model tfidf_lr --save

# Treinar Word2Vec + LR
uv run python src/ep1/train.py --dataset arcaico_moderno --model word2vec_lr --save

# Comparar resultados no terminal ou em results.csv
```

## ğŸ“¦ DependÃªncias

Foi adicionada a biblioteca `gensim` ao projeto:

```bash
uv add gensim
```

O `pyproject.toml` agora inclui:
```toml
dependencies = [
    "gensim>=4.3.3",
    ...
]
```

## ğŸ“ Quando usar Word2Vec?

**Use Word2Vec quando:**
- âœ… VocÃª tem um **corpus especializado** (termos tÃ©cnicos, domÃ­nio especÃ­fico)
- âœ… Quer capturar **relaÃ§Ãµes semÃ¢nticas** entre palavras
- âœ… Tem **dados suficientes** (pelo menos alguns milhares de documentos)
- âœ… Pode aceitar **maior tempo de treinamento**

**Use TF-IDF quando:**
- âœ… Quer um modelo **mais rÃ¡pido**
- âœ… Precisa de **interpretabilidade** (ver quais palavras sÃ£o importantes)
- âœ… Tem **pouco tempo para experimentaÃ§Ã£o**
- âœ… Dataset Ã© muito **pequeno** (< 1000 documentos)

## ğŸ”¬ PrÃ³ximos Passos

1. **Experimentar**: Rode `uv run python example_word2vec.py`
2. **Treinar**: Execute experimentos com seus datasets
3. **Comparar**: Veja se Word2Vec performa melhor que TF-IDF
4. **Otimizar**: Ajuste os parÃ¢metros do grid search conforme necessÃ¡rio

## ğŸ“š ReferÃªncias

- [Gensim Word2Vec Documentation](https://radimrehurek.com/gensim/models/word2vec.html)
- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)
- [CBOW vs Skip-gram](https://towardsdatascience.com/word2vec-skip-gram-and-cbow-4c6f6d5e6f7e)
